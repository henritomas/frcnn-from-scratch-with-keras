{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2019-12-27 19:51:04.246419: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-12-27 19:51:07.008463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-12-27 19:51:07.031143: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5586a4623ff0 executing computations on platform CUDA. Devices:\n",
      "2019-12-27 19:51:07.031188: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-12-27 19:51:07.514447: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-12-27 19:51:07.514722: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5586a468ca60 executing computations on platform Host. Devices:\n",
      "2019-12-27 19:51:07.514763: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-12-27 19:51:07.515156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2019-12-27 19:51:07.515196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-12-27 19:51:07.549833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-12-27 19:51:07.549871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-12-27 19:51:07.549891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-12-27 19:51:07.550263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10801 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Parsing annotation files\n",
      "{'person': 2358}\n",
      "Training images per class:\n",
      "{'bg': 0, 'person': 2358}\n",
      "Num classes (including bg) = 2\n",
      "Config has been written to config.pickle, and can be loaded when testing to ensure correct results\n",
      "Num train samples 840\n",
      "Num val samples 185\n",
      "WARNING:tensorflow:From /home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "loading weights from ./pretrain/mobilenet_1_0_224_tf.h5\n",
      "loaded weights!\n",
      "Starting training\n",
      "Epoch 1/40\n",
      "1000/1000 [==============================] - 310s 310ms/step - loss: 6.4929 - rpn_out_class_loss: 6.2770 - rpn_out_regress_loss: 0.2159 - val_loss: 6.1603 - val_rpn_out_class_loss: 5.9546 - val_rpn_out_regress_loss: 0.2057\n",
      "Epoch 2/40\n",
      "1000/1000 [==============================] - 278s 278ms/step - loss: 5.8922 - rpn_out_class_loss: 5.6744 - rpn_out_regress_loss: 0.2179 - val_loss: 5.3903 - val_rpn_out_class_loss: 5.1450 - val_rpn_out_regress_loss: 0.2452\n",
      "Epoch 3/40\n",
      "1000/1000 [==============================] - 273s 273ms/step - loss: 4.7243 - rpn_out_class_loss: 4.5302 - rpn_out_regress_loss: 0.1942 - val_loss: 4.0241 - val_rpn_out_class_loss: 3.8346 - val_rpn_out_regress_loss: 0.1895\n",
      "Epoch 4/40\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 3.7379 - rpn_out_class_loss: 3.5498 - rpn_out_regress_loss: 0.1881 - val_loss: 3.4089 - val_rpn_out_class_loss: 3.2011 - val_rpn_out_regress_loss: 0.2078\n",
      "\n",
      "Epoch 00004: loss improved from inf to 3.73791, saving model to ./models/rpn/rpn.thundernetv1.weights.04-3.74.hdf5\n",
      "Epoch 5/40\n",
      "1000/1000 [==============================] - 273s 273ms/step - loss: 2.9536 - rpn_out_class_loss: 2.7829 - rpn_out_regress_loss: 0.1708 - val_loss: 2.8390 - val_rpn_out_class_loss: 2.6601 - val_rpn_out_regress_loss: 0.1789\n",
      "Epoch 6/40\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 2.3860 - rpn_out_class_loss: 2.2140 - rpn_out_regress_loss: 0.1720 - val_loss: 2.3008 - val_rpn_out_class_loss: 2.1121 - val_rpn_out_regress_loss: 0.1887\n",
      "Epoch 7/40\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 2.0686 - rpn_out_class_loss: 1.8982 - rpn_out_regress_loss: 0.1704 - val_loss: 1.9176 - val_rpn_out_class_loss: 1.7411 - val_rpn_out_regress_loss: 0.1765\n",
      "Epoch 8/40\n",
      "1000/1000 [==============================] - 275s 275ms/step - loss: 1.7996 - rpn_out_class_loss: 1.6331 - rpn_out_regress_loss: 0.1665 - val_loss: 2.0211 - val_rpn_out_class_loss: 1.8304 - val_rpn_out_regress_loss: 0.1906\n",
      "\n",
      "Epoch 00008: loss improved from 3.73791 to 1.79958, saving model to ./models/rpn/rpn.thundernetv1.weights.08-1.80.hdf5\n",
      "Epoch 9/40\n",
      "1000/1000 [==============================] - 277s 277ms/step - loss: 1.6665 - rpn_out_class_loss: 1.4957 - rpn_out_regress_loss: 0.1708 - val_loss: 1.8295 - val_rpn_out_class_loss: 1.6509 - val_rpn_out_regress_loss: 0.1785\n",
      "Epoch 10/40\n",
      "1000/1000 [==============================] - 276s 276ms/step - loss: 1.4522 - rpn_out_class_loss: 1.2900 - rpn_out_regress_loss: 0.1623 - val_loss: 1.7897 - val_rpn_out_class_loss: 1.6046 - val_rpn_out_regress_loss: 0.1851\n",
      "Epoch 11/40\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 1.4019 - rpn_out_class_loss: 1.2365 - rpn_out_regress_loss: 0.1654 - val_loss: 1.7692 - val_rpn_out_class_loss: 1.5913 - val_rpn_out_regress_loss: 0.1778\n",
      "Epoch 12/40\n",
      "1000/1000 [==============================] - 279s 279ms/step - loss: 1.2712 - rpn_out_class_loss: 1.1106 - rpn_out_regress_loss: 0.1606 - val_loss: 1.9295 - val_rpn_out_class_loss: 1.7519 - val_rpn_out_regress_loss: 0.1776\n",
      "\n",
      "Epoch 00012: loss improved from 1.79958 to 1.27119, saving model to ./models/rpn/rpn.thundernetv1.weights.12-1.27.hdf5\n",
      "Epoch 13/40\n",
      " 438/1000 [============>.................] - ETA: 2:29 - loss: 1.2770 - rpn_out_class_loss: 1.1120 - rpn_out_regress_loss: 0.1650^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train_rpn.py\", line 244, in <module>\n",
      "    steps_per_epoch=options.epoch_length, callbacks=callback, validation_steps=100)\n",
      "  File \"/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/keras/engine/training.py\", line 1418, in fit_generator\n"
     ]
    }
   ],
   "source": [
    "#INITIAL TRAIN RPN FIRST\n",
    "BACKBONE = \"thundernetv1\"\n",
    "DATASET = \"/home/henri_tomas/projects/voc07/2007_train.txt\"\n",
    "\n",
    "!python train_rpn.py --network $BACKBONE -o simple -p $DATASET -n 4 --num_epochs 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2019-12-29 20:31:20.159804: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-12-29 20:31:22.281394: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-12-29 20:31:22.281988: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x556c12d84e20 executing computations on platform CUDA. Devices:\n",
      "2019-12-29 20:31:22.282026: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-12-29 20:31:22.284746: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
      "2019-12-29 20:31:22.285058: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x556c12ded890 executing computations on platform Host. Devices:\n",
      "2019-12-29 20:31:22.285096: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-12-29 20:31:22.285443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2019-12-29 20:31:22.285489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-12-29 20:31:22.286639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-12-29 20:31:22.286678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-12-29 20:31:22.286701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-12-29 20:31:22.287006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Parsing annotation files\n",
      "Training images per class:\n",
      "{'bg': 0, 'person': 2358}\n",
      "Num classes (including bg) = 2\n",
      "Config has been written to config.pickle, and can be loaded when testing to ensure correct results\n",
      "Num train samples 866\n",
      "Num val samples 159\n",
      "WARNING:tensorflow:From /home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "loading weights from ./pretrain/mobilenet_1_0_224_tf.h5\n",
      "loading RPN weights from  ./models/rpn/rpn.thundernetv1.weights.12-1.27.hdf5\n",
      "Starting training\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-12-29 20:31:42.851011: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
      "  19/1000 [..............................] - ETA: 22:45 - rpn_cls: 2.0575 - rpn_regr: 0.2247 - detector_cls: 0.7041 - detector_regr: 0.4931 - average number of objects: 1.9474^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train_frcnn.py\", line 289, in <module>\n",
      "    loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
      "  File \"/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/keras/engine/training.py\", line 1217, in train_on_batch\n",
      "    outputs = self.train_function(ins)\n",
      "  File \"/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2715, in __call__\n",
      "    return self._call(inputs)\n",
      "  File \"/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2675, in _call\n",
      "    fetched = self._callable_fn(*array_vals)\n",
      "  File \"/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1439, in __call__\n",
      "    run_metadata_ptr)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#INITIAL TRAINING FRCNN, NOT CONTINUE TRAINING\n",
    "\n",
    "BACKBONE = \"thundernetv1\"\n",
    "DATASET = \"/home/henri_tomas/projects/voc07/2007_train.txt\"\n",
    "RPN = \"./models/rpn/rpn.thundernetv1.weights.12-1.27.hdf5\"\n",
    "SAVE_TO = \"voc_person_psroi\"\n",
    "\n",
    "!python train_frcnn.py --network $BACKBONE -o simple -p $DATASET --rpn $RPN --num_epochs 10 --dataset $SAVE_TO -n 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2019-12-26 19:02:32.466253: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-12-26 19:02:35.305465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-12-26 19:02:35.365250: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55e9b010c960 executing computations on platform CUDA. Devices:\n",
      "2019-12-26 19:02:35.365293: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-12-26 19:02:35.892058: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-12-26 19:02:35.892338: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55e9b0175400 executing computations on platform Host. Devices:\n",
      "2019-12-26 19:02:35.892390: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-12-26 19:02:35.892868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2019-12-26 19:02:35.892906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-12-26 19:02:35.930585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-12-26 19:02:35.930618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-12-26 19:02:35.930633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-12-26 19:02:35.930986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "Parsing annotation files\n",
      "Training images per class:\n",
      "{'bg': 0, 'person': 2358}\n",
      "Num classes (including bg) = 2\n",
      "Config has been written to config.pickle, and can be loaded when testing to ensure correct results\n",
      "Num train samples 850\n",
      "Num val samples 175\n",
      "WARNING:tensorflow:From /home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "loading weights from ./pretrain/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
      "loading previous model from  ./models/mobilenetv2/voc_person.hdf5\n",
      "Starting training\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From /home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "2019-12-26 19:04:05.260106: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
      "1000/1000 [==============================] - 964s 964ms/step - rpn_cls: 2.6657 - rpn_regr: 0.1798 - detector_cls: 0.4986 - detector_regr: 0.3127 - average number of objects: 0.6540\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 1.094\n",
      "Classifier accuracy for bounding boxes from RPN: 0.765\n",
      "Loss RPN classifier: 2.640772760563674\n",
      "Loss RPN regression: 0.183447440831922\n",
      "Loss Detector classifier: 0.4963710466474295\n",
      "Loss Detector regression: 0.30898188017029315\n",
      "Elapsed time: 963.9478027820587\n",
      "Total loss decreased from inf to 3.6295731282133183, saving weights\n",
      "Epoch 2/10\n",
      "Average number of overlapping bounding boxes from RPN = 1.094 for 1000 previous iterations\n",
      " 999/1000 [============================>.] - ETA: 0s - rpn_cls: 2.2748 - rpn_regr: 0.1757 - detector_cls: 0.4814 - detector_regr: 0.3269 - average number of objects: 0.6847Average number of overlapping bounding boxes from RPN = 1.106 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 788s 788ms/step - rpn_cls: 2.2747 - rpn_regr: 0.1757 - detector_cls: 0.4814 - detector_regr: 0.3269 - average number of objects: 0.6840\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 1.1048951048951048\n",
      "Classifier accuracy for bounding boxes from RPN: 0.763\n",
      "Loss RPN classifier: 2.212003843069301\n",
      "Loss RPN regression: 0.17431541379075496\n",
      "Loss Detector classifier: 0.4940381983742118\n",
      "Loss Detector regression: 0.32449270809302105\n",
      "Elapsed time: 813.2641212940216\n",
      "Total loss decreased from 3.6295731282133183 to 3.204850163327289, saving weights\n",
      "Epoch 3/10\n",
      " 999/1000 [============================>.] - ETA: 0s - rpn_cls: 2.2935 - rpn_regr: 0.1751 - detector_cls: 0.4699 - detector_regr: 0.3108 - average number of objects: 0.6587Average number of overlapping bounding boxes from RPN = 1.129 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 791s 791ms/step - rpn_cls: 2.2934 - rpn_regr: 0.1750 - detector_cls: 0.4699 - detector_regr: 0.3108 - average number of objects: 0.6580\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 1.129\n",
      "Classifier accuracy for bounding boxes from RPN: 0.7785\n",
      "Loss RPN classifier: 2.19338484545646\n",
      "Loss RPN regression: 0.17183054960612207\n",
      "Loss Detector classifier: 0.4746081429682672\n",
      "Loss Detector regression: 0.3068461253400892\n",
      "Elapsed time: 790.9623637199402\n",
      "Total loss decreased from 3.204850163327289 to 3.1466696633709383, saving weights\n",
      "Epoch 4/10\n",
      " 999/1000 [============================>.] - ETA: 0s - rpn_cls: 2.2085 - rpn_regr: 0.1784 - detector_cls: 0.4735 - detector_regr: 0.3257 - average number of objects: 0.6877Average number of overlapping bounding boxes from RPN = 1.156 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 779s 779ms/step - rpn_cls: 2.2085 - rpn_regr: 0.1784 - detector_cls: 0.4735 - detector_regr: 0.3257 - average number of objects: 0.6880\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 1.158\n",
      "Classifier accuracy for bounding boxes from RPN: 0.781\n",
      "Loss RPN classifier: 2.187171300798911\n",
      "Loss RPN regression: 0.172286939015612\n",
      "Loss Detector classifier: 0.4652842021882534\n",
      "Loss Detector regression: 0.32133134231343863\n",
      "Elapsed time: 779.1784372329712\n",
      "Total loss decreased from 3.1466696633709383 to 3.146073784316215, saving weights\n",
      "Epoch 5/10\n",
      " 999/1000 [============================>.] - ETA: 0s - rpn_cls: 1.9754 - rpn_regr: 0.1660 - detector_cls: 0.4631 - detector_regr: 0.3462 - average number of objects: 0.7057Average number of overlapping bounding boxes from RPN = 1.201 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 775s 775ms/step - rpn_cls: 1.9754 - rpn_regr: 0.1660 - detector_cls: 0.4631 - detector_regr: 0.3462 - average number of objects: 0.7060\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 1.201\n",
      "Classifier accuracy for bounding boxes from RPN: 0.7715\n",
      "Loss RPN classifier: 2.039065601530783\n",
      "Loss RPN regression: 0.163855867120903\n",
      "Loss Detector classifier: 0.4757479890659452\n",
      "Loss Detector regression: 0.33992529987264425\n",
      "Elapsed time: 775.0349419116974\n",
      "Total loss decreased from 3.146073784316215 to 3.018594757590275, saving weights\n",
      "Epoch 6/10\n",
      " 999/1000 [============================>.] - ETA: 0s - rpn_cls: 1.9843 - rpn_regr: 0.1569 - detector_cls: 0.4711 - detector_regr: 0.3172 - average number of objects: 0.6697Average number of overlapping bounding boxes from RPN = 1.129 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 775s 775ms/step - rpn_cls: 1.9842 - rpn_regr: 0.1569 - detector_cls: 0.4711 - detector_regr: 0.3172 - average number of objects: 0.6700\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 1.128\n",
      "Classifier accuracy for bounding boxes from RPN: 0.773\n",
      "Loss RPN classifier: 1.9334424436846485\n",
      "Loss RPN regression: 0.15814248129888436\n",
      "Loss Detector classifier: 0.46859310459345577\n",
      "Loss Detector regression: 0.3066979504758492\n",
      "Elapsed time: 775.6281087398529\n",
      "Total loss decreased from 3.018594757590275 to 2.8668759800528383, saving weights\n",
      "Epoch 7/10\n",
      " 999/1000 [============================>.] - ETA: 0s - rpn_cls: 1.7787 - rpn_regr: 0.1532 - detector_cls: 0.4331 - detector_regr: 0.3190 - average number of objects: 0.6977Average number of overlapping bounding boxes from RPN = 1.169 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 773s 773ms/step - rpn_cls: 1.7788 - rpn_regr: 0.1532 - detector_cls: 0.4331 - detector_regr: 0.3190 - average number of objects: 0.6970\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 1.168\n",
      "Classifier accuracy for bounding boxes from RPN: 0.7905\n",
      "Loss RPN classifier: 1.8053326540711918\n",
      "Loss RPN regression: 0.15882076935004444\n",
      "Loss Detector classifier: 0.4333363865464926\n",
      "Loss Detector regression: 0.3255530558570754\n",
      "Elapsed time: 773.855174779892\n",
      "Total loss decreased from 2.8668759800528383 to 2.7230428658248043, saving weights\n",
      "Epoch 8/10\n",
      " 999/1000 [============================>.] - ETA: 0s - rpn_cls: 1.8487 - rpn_regr: 0.1516 - detector_cls: 0.4564 - detector_regr: 0.3239 - average number of objects: 0.7117Average number of overlapping bounding boxes from RPN = 1.217 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 777s 777ms/step - rpn_cls: 1.8486 - rpn_regr: 0.1516 - detector_cls: 0.4564 - detector_regr: 0.3239 - average number of objects: 0.7120\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 1.218\n",
      "Classifier accuracy for bounding boxes from RPN: 0.789\n",
      "Loss RPN classifier: 1.738810913824705\n",
      "Loss RPN regression: 0.15418267215555534\n",
      "Loss Detector classifier: 0.44621953473985193\n",
      "Loss Detector regression: 0.33562510498054327\n",
      "Elapsed time: 777.7685830593109\n",
      "Total loss decreased from 2.7230428658248043 to 2.6748382257006553, saving weights\n",
      "Epoch 9/10\n",
      " 999/1000 [============================>.] - ETA: 0s - rpn_cls: 1.8908 - rpn_regr: 0.1498 - detector_cls: 0.4699 - detector_regr: 0.3172 - average number of objects: 0.6557Average number of overlapping bounding boxes from RPN = 1.175 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 780s 780ms/step - rpn_cls: 1.8906 - rpn_regr: 0.1498 - detector_cls: 0.4699 - detector_regr: 0.3172 - average number of objects: 0.6560\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 1.176\n",
      "Classifier accuracy for bounding boxes from RPN: 0.785\n",
      "Loss RPN classifier: 1.775350940182862\n",
      "Loss RPN regression: 0.1542937635818962\n",
      "Loss Detector classifier: 0.4626076247394085\n",
      "Loss Detector regression: 0.3110596912642941\n",
      "Elapsed time: 780.6047685146332\n",
      "Epoch 10/10\n",
      " 999/1000 [============================>.] - ETA: 0s - rpn_cls: 1.7796 - rpn_regr: 0.1588 - detector_cls: 0.4777 - detector_regr: 0.3035 - average number of objects: 0.7117Average number of overlapping bounding boxes from RPN = 1.202 for 1000 previous iterations\n",
      "1000/1000 [==============================] - 776s 776ms/step - rpn_cls: 1.7795 - rpn_regr: 0.1588 - detector_cls: 0.4777 - detector_regr: 0.3035 - average number of objects: 0.7120\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 1.201\n",
      "Classifier accuracy for bounding boxes from RPN: 0.786\n",
      "Loss RPN classifier: 1.6889840393664375\n",
      "Loss RPN regression: 0.15703617807105183\n",
      "Loss Detector classifier: 0.44940934962779283\n",
      "Loss Detector regression: 0.3172746745571494\n",
      "Elapsed time: 776.0653512477875\n",
      "Total loss decreased from 2.6748382257006553 to 2.6127042416224318, saving weights\n",
      "Training complete, exiting.\n"
     ]
    }
   ],
   "source": [
    "# CONTINUE TRAINING\n",
    "BACKBONE = \"thundernetv1\"\n",
    "DATASET = \"/home/henri_tomas/projects/voc07/2007_train.txt\"\n",
    "SAVE_TO = \"voc_person_psroi\"\n",
    "MODEL = \"./models/thundernetv1/voc_person_rpn.hdf5\"\n",
    "\n",
    "!python train_frcnn.py --network $BACKBONE -o simple -p $DATASET --num_epochs 10 --dataset $SAVE_TO -n 4 --load $MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2019-12-30 19:00:57.885536: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-12-30 19:00:59.930099: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-12-30 19:00:59.930853: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5606a13d3d90 executing computations on platform CUDA. Devices:\n",
      "2019-12-30 19:00:59.930887: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-12-30 19:00:59.933683: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-12-30 19:00:59.933912: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5606a143c810 executing computations on platform Host. Devices:\n",
      "2019-12-30 19:00:59.933948: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-12-30 19:00:59.934325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2019-12-30 19:00:59.934368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-12-30 19:00:59.935284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-12-30 19:00:59.935315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-12-30 19:00:59.935340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-12-30 19:00:59.935619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "{0: 'person', 1: 'bg'}\n",
      "backbone is not resnet50. number of features chosen is 512\n",
      "WARNING:tensorflow:From /home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Loading weights from ./models/thundernetv1/voc_person_rpn.hdf5\n",
      "dog.jpg\n",
      "2019-12-30 19:01:05.156539: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
      "Full inference = 1.873065710067749\n",
      "[('person', 79.90808486938477), ('person', 79.90573644638062)]\n",
      "{'person': [[0, 160, 448, 576], [144, 256, 272, 544]]}\n",
      "dog2.jpg\n",
      "Full inference = 0.5865976810455322\n",
      "[('person', 80.44861555099487), ('person', 72.898930311203), ('person', 70.62073349952698)]\n",
      "{'person': [[176, 0, 688, 576], [0, 208, 464, 576], [352, 112, 608, 416]]}\n",
      "person.jpg\n",
      "Full inference = 0.6148877143859863\n",
      "[('person', 85.27583479881287), ('person', 79.17191982269287)]\n",
      "{'person': [[336, 112, 592, 544], [96, 320, 880, 576]]}\n",
      "person2.jpg\n",
      "Full inference = 0.589669942855835\n",
      "[('person', 85.75233221054077), ('person', 81.47757053375244)]\n",
      "{'person': [[0, 0, 384, 512], [656, 208, 784, 496]]}\n",
      "person3.jpg\n",
      "Full inference = 0.5977268218994141\n",
      "[('person', 85.82739233970642), ('person', 76.43563747406006), ('person', 76.29464864730835), ('person', 72.60004878044128), ('person', 71.19268774986267)]\n",
      "{'person': [[160, 128, 400, 528], [160, 208, 272, 464], [304, 96, 432, 352], [448, 160, 512, 304], [560, 192, 800, 576]]}\n"
     ]
    }
   ],
   "source": [
    "#TEST ON TEST_IMAGES\n",
    "\n",
    "BACKBONE = \"thundernetv1\"\n",
    "TEST_IMAGES = \"./test_images/\"\n",
    "MODEL = \"./models/thundernetv1/voc_person_rpn.hdf5\"\n",
    "!python test_frcnn.py --network $BACKBONE -p $TEST_IMAGES --load $MODEL --write -n 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "2019-12-29 20:00:09.491740: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-12-29 20:00:11.560411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-12-29 20:00:11.561083: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55bad255e210 executing computations on platform CUDA. Devices:\n",
      "2019-12-29 20:00:11.561121: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2019-12-29 20:00:11.563838: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
      "2019-12-29 20:00:11.564074: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55bad25c6c90 executing computations on platform Host. Devices:\n",
      "2019-12-29 20:00:11.564108: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-12-29 20:00:11.564443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
      "2019-12-29 20:00:11.564518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-12-29 20:00:11.565490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-12-29 20:00:11.565519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-12-29 20:00:11.565532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-12-29 20:00:11.565769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
      "{0: 'person', 1: 'bg'}\n",
      "backbone is not resnet50. number of features chosen is 512\n",
      "WARNING:tensorflow:From /home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Loading weights from ./models/thundernetv1/voc_person_rpn.hdf5\n"
     ]
    }
   ],
   "source": [
    "#SAVE MODELS TO H5 WITH CORRECT, CONSTANT INPUT_SHAPES !!! \n",
    "\n",
    "BACKBONE = \"thundernetv1\"\n",
    "TEST_IMAGES = \"./test_images/\"\n",
    "MODEL = \"./models/thundernetv1/voc_person_rpn.hdf5\"\n",
    "!python tflite_convert.py --network $BACKBONE -p $TEST_IMAGES --load $MODEL --write -n 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:From /home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/lite/python/lite.py:591: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 122 variables.\n",
      "INFO:tensorflow:Converted 122 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7058712"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CONVERT RPN\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(\"model_rpn.h5\")\n",
    "tflite_model = converter.convert()\n",
    "open(\"model_rpn.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'TFLiteConverterV2' has no attribute 'from_keras_model_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ab8fcadf39ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"RoiPoolingConv\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mRoiPoolingConv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m }\n\u001b[0;32m----> 7\u001b[0;31m converter = tf.lite.TFLiteConverter.from_keras_model_file(\"model_cls.h5\",\n\u001b[0m\u001b[1;32m      8\u001b[0m                                                           custom_objects=custom_objects)\n\u001b[1;32m      9\u001b[0m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'TFLiteConverterV2' has no attribute 'from_keras_model_file'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras_frcnn.RoiPoolingConv import RoiPoolingConv\n",
    "\n",
    "custom_objects = {\n",
    "    \"RoiPoolingConv\" : RoiPoolingConv,\n",
    "}\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(\"model_cls.h5\",\n",
    "                                                          custom_objects=custom_objects)\n",
    "tflite_model = converter.convert()\n",
    "open(\"model_cls.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Node' object has no attribute 'output_masks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5e4f569a3c90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    144\u001b[0m   if (h5py is not None and (\n\u001b[1;32m    145\u001b[0m       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     model = model_config_lib.model_from_config(model_config,\n\u001b[0;32m--> 168\u001b[0;31m                                                custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;31m# set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m       printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    189\u001b[0m             custom_objects=dict(\n\u001b[1;32m    190\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_GLOBAL_CUSTOM_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    192\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    904\u001b[0m     \"\"\"\n\u001b[1;32m    905\u001b[0m     input_tensors, output_tensors, created_layers = reconstruct_from_config(\n\u001b[0;32m--> 906\u001b[0;31m         config, custom_objects)\n\u001b[0m\u001b[1;32m    907\u001b[0m     model = cls(inputs=input_tensors, outputs=output_tensors,\n\u001b[1;32m    908\u001b[0m                 name=config.get('name'))\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mreconstruct_from_config\u001b[0;34m(config, custom_objects, created_layers)\u001b[0m\n\u001b[1;32m   1850\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munprocessed_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m           \u001b[0mprocess_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m   \u001b[0minput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mprocess_node\u001b[0;34m(layer, node_data)\u001b[0m\n\u001b[1;32m   1797\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_input_tensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m         \u001b[0minput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_input_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1799\u001b[0;31m       \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m       \u001b[0;31m# Update node index map.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Handle mask propagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mprevious_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collect_previous_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             \u001b[0muser_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_all_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_collect_previous_mask\u001b[0;34m(input_tensors)\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0minbound_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m             \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minbound_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1311\u001b[0;31m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1312\u001b[0m             \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Node' object has no attribute 'output_masks'"
     ]
    }
   ],
   "source": [
    "#CONVERT CLS\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras_frcnn.RoiPoolingConv import RoiPoolingConv\n",
    "\n",
    "keras_file = \"model_cls.h5\"\n",
    "\n",
    "# Convert the model.\n",
    "custom_objects = {\n",
    "    \"RoiPoolingConv\" : RoiPoolingConv,\n",
    "}\n",
    "\n",
    "model = tf.keras.models.load_model(keras_file, custom_objects=custom_objects)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(\"model_cls.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/henri_tomas/miniconda3/envs/keras1/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#Sanity Test (does tf_RPN and tflite_RPN have same output?)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf_model = tf.keras.models.load_model(\"model_rpn.h5\")\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model_rpn_320.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF and TFLITE models are equal up to 5 decimal places\n"
     ]
    }
   ],
   "source": [
    "#0=cls, 1=reg, 2=base_layers\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Test the original TensorFlow model on random input data.\n",
    "#tf_results = tf_model(tf.constant(input_data)) #produces a tf tensor which doesnt show the FUCKING values\n",
    "tf_results = tf_model.predict(input_data)       #shows values because numpy array yey \n",
    "#print(tflite_results.shape)\n",
    "    \n",
    "# Compare the result.\n",
    "# NOTE: only 0 (the cls layer) is tested here, pray that is enough proof\n",
    "for tf_result, tflite_result in zip(tf_results[0], tflite_results):\n",
    "    decimal = 5\n",
    "    error = np.testing.assert_almost_equal(tf_result, tflite_result, decimal=decimal)\n",
    "    \n",
    "    #IF no assertion error raised...\n",
    "    if error==None:\n",
    "        print(\"TF and TFLITE models are equal up to {} decimal places\".format(decimal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotation files\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000036.jpg', 'width': 332, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 27, 'x2': 319, 'y1': 79, 'y2': 344}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000078.jpg', 'width': 500, 'height': 419, 'bboxes': [{'class': 'dog', 'x1': 15, 'x2': 475, 'y1': 75, 'y2': 412}, {'class': 'dog', 'x1': 94, 'x2': 437, 'y1': 41, 'y2': 238}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000112.jpg', 'width': 333, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 70, 'x2': 277, 'y1': 174, 'y2': 328}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000140.jpg', 'width': 500, 'height': 401, 'bboxes': [{'class': 'dog', 'x1': 107, 'x2': 386, 'y1': 146, 'y2': 300}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000171.jpg', 'width': 375, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 128, 'y1': 290, 'y2': 407}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000278.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 294, 'x2': 500, 'y1': 156, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000359.jpg', 'width': 432, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 263, 'x2': 388, 'y1': 266, 'y2': 479}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000454.jpg', 'width': 500, 'height': 297, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 460, 'y1': 15, 'y2': 297}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000537.jpg', 'width': 500, 'height': 400, 'bboxes': [{'class': 'dog', 'x1': 111, 'x2': 396, 'y1': 12, 'y2': 400}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000609.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 16, 'x2': 460, 'y1': 134, 'y2': 275}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000654.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 295, 'x2': 382, 'y1': 178, 'y2': 243}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000699.jpg', 'width': 500, 'height': 381, 'bboxes': [{'class': 'dog', 'x1': 29, 'x2': 337, 'y1': 205, 'y2': 377}, {'class': 'dog', 'x1': 102, 'x2': 366, 'y1': 27, 'y2': 358}, {'class': 'dog', 'x1': 309, 'x2': 472, 'y1': 130, 'y2': 362}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000805.jpg', 'width': 375, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 29, 'x2': 356, 'y1': 65, 'y2': 411}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000849.jpg', 'width': 200, 'height': 368, 'bboxes': [{'class': 'dog', 'x1': 23, 'x2': 159, 'y1': 56, 'y2': 328}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000902.jpg', 'width': 500, 'height': 399, 'bboxes': [{'class': 'dog', 'x1': 71, 'x2': 332, 'y1': 85, 'y2': 314}, {'class': 'dog', 'x1': 271, 'x2': 499, 'y1': 75, 'y2': 210}, {'class': 'dog', 'x1': 160, 'x2': 313, 'y1': 34, 'y2': 105}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000919.jpg', 'width': 500, 'height': 331, 'bboxes': [{'class': 'dog', 'x1': 39, 'x2': 352, 'y1': 82, 'y2': 330}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000958.jpg', 'width': 400, 'height': 300, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 327, 'y1': 90, 'y2': 299}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/000965.jpg', 'width': 375, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 94, 'x2': 237, 'y1': 285, 'y2': 461}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001014.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 278, 'x2': 395, 'y1': 188, 'y2': 375}, {'class': 'dog', 'x1': 150, 'x2': 337, 'y1': 253, 'y2': 369}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001127.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 42, 'x2': 203, 'y1': 125, 'y2': 330}, {'class': 'dog', 'x1': 195, 'x2': 295, 'y1': 136, 'y2': 288}, {'class': 'dog', 'x1': 283, 'x2': 496, 'y1': 113, 'y2': 241}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001140.jpg', 'width': 375, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 257, 'x2': 323, 'y1': 312, 'y2': 407}, {'class': 'dog', 'x1': 57, 'x2': 157, 'y1': 298, 'y2': 465}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001151.jpg', 'width': 500, 'height': 331, 'bboxes': [{'class': 'dog', 'x1': 93, 'x2': 316, 'y1': 109, 'y2': 325}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001209.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 196, 'x2': 273, 'y1': 96, 'y2': 238}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001226.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 105, 'x2': 499, 'y1': 50, 'y2': 274}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001234.jpg', 'width': 260, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 62, 'y1': 425, 'y2': 475}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001239.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 108, 'x2': 320, 'y1': 60, 'y2': 160}, {'class': 'dog', 'x1': 228, 'x2': 381, 'y1': 138, 'y2': 375}, {'class': 'dog', 'x1': 118, 'x2': 286, 'y1': 94, 'y2': 307}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001406.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 109, 'x2': 404, 'y1': 127, 'y2': 373}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001418.jpg', 'width': 405, 'height': 480, 'bboxes': [{'class': 'dog', 'x1': 32, 'x2': 344, 'y1': 34, 'y2': 473}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001517.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 19, 'x2': 416, 'y1': 109, 'y2': 291}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001524.jpg', 'width': 500, 'height': 370, 'bboxes': [{'class': 'dog', 'x1': 185, 'x2': 216, 'y1': 158, 'y2': 230}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001594.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 253, 'x2': 302, 'y1': 177, 'y2': 238}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001607.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 21, 'x2': 500, 'y1': 1, 'y2': 374}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001643.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 472, 'y1': 42, 'y2': 323}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001680.jpg', 'width': 375, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 91, 'x2': 331, 'y1': 104, 'y2': 500}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001738.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 125, 'x2': 323, 'y1': 68, 'y2': 300}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001759.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 2, 'x2': 197, 'y1': 153, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001832.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 416, 'y1': 78, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/001985.jpg', 'width': 333, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 81, 'x2': 238, 'y1': 92, 'y2': 486}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002034.jpg', 'width': 500, 'height': 288, 'bboxes': [{'class': 'dog', 'x1': 272, 'x2': 474, 'y1': 69, 'y2': 199}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002055.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 15, 'x2': 151, 'y1': 248, 'y2': 368}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002116.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 177, 'x2': 425, 'y1': 206, 'y2': 373}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002117.jpg', 'width': 375, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 79, 'x2': 202, 'y1': 281, 'y2': 451}, {'class': 'dog', 'x1': 106, 'x2': 250, 'y1': 128, 'y2': 297}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002199.jpg', 'width': 500, 'height': 372, 'bboxes': [{'class': 'dog', 'x1': 69, 'x2': 477, 'y1': 163, 'y2': 372}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002277.jpg', 'width': 383, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 297, 'y1': 60, 'y2': 500}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002287.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 11, 'x2': 466, 'y1': 65, 'y2': 319}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002315.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 177, 'x2': 394, 'y1': 37, 'y2': 249}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002359.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 12, 'x2': 351, 'y1': 114, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002384.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 210, 'x2': 442, 'y1': 110, 'y2': 369}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002443.jpg', 'width': 333, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 51, 'x2': 333, 'y1': 175, 'y2': 393}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002466.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 125, 'x2': 421, 'y1': 92, 'y2': 217}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002512.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 82, 'x2': 298, 'y1': 40, 'y2': 296}, {'class': 'dog', 'x1': 258, 'x2': 450, 'y1': 128, 'y2': 347}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002609.jpg', 'width': 500, 'height': 342, 'bboxes': [{'class': 'dog', 'x1': 40, 'x2': 217, 'y1': 40, 'y2': 255}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002611.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 152, 'x2': 337, 'y1': 131, 'y2': 298}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002680.jpg', 'width': 500, 'height': 334, 'bboxes': [{'class': 'dog', 'x1': 4, 'x2': 500, 'y1': 36, 'y2': 334}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002691.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 192, 'y1': 232, 'y2': 365}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002697.jpg', 'width': 480, 'height': 498, 'bboxes': [{'class': 'dog', 'x1': 25, 'x2': 216, 'y1': 26, 'y2': 498}, {'class': 'dog', 'x1': 199, 'x2': 422, 'y1': 42, 'y2': 404}, {'class': 'dog', 'x1': 241, 'x2': 480, 'y1': 375, 'y2': 498}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002826.jpg', 'width': 333, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 211, 'x2': 333, 'y1': 26, 'y2': 183}, {'class': 'dog', 'x1': 1, 'x2': 202, 'y1': 60, 'y2': 196}, {'class': 'dog', 'x1': 47, 'x2': 230, 'y1': 242, 'y2': 458}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/002957.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 158, 'x2': 348, 'y1': 157, 'y2': 240}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003100.jpg', 'width': 400, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 49, 'x2': 175, 'y1': 263, 'y2': 330}, {'class': 'dog', 'x1': 220, 'x2': 294, 'y1': 264, 'y2': 319}, {'class': 'dog', 'x1': 214, 'x2': 269, 'y1': 243, 'y2': 286}, {'class': 'dog', 'x1': 153, 'x2': 228, 'y1': 278, 'y2': 322}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003110.jpg', 'width': 500, 'height': 358, 'bboxes': [{'class': 'dog', 'x1': 213, 'x2': 275, 'y1': 127, 'y2': 266}, {'class': 'dog', 'x1': 121, 'x2': 188, 'y1': 107, 'y2': 234}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003147.jpg', 'width': 500, 'height': 332, 'bboxes': [{'class': 'dog', 'x1': 153, 'x2': 366, 'y1': 145, 'y2': 257}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003161.jpg', 'width': 500, 'height': 328, 'bboxes': [{'class': 'dog', 'x1': 113, 'x2': 157, 'y1': 161, 'y2': 224}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003253.jpg', 'width': 500, 'height': 430, 'bboxes': [{'class': 'dog', 'x1': 8, 'x2': 249, 'y1': 126, 'y2': 405}, {'class': 'dog', 'x1': 313, 'x2': 500, 'y1': 19, 'y2': 304}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003284.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 345, 'x2': 443, 'y1': 303, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003339.jpg', 'width': 432, 'height': 288, 'bboxes': [{'class': 'dog', 'x1': 21, 'x2': 314, 'y1': 24, 'y2': 288}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003343.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 167, 'x2': 407, 'y1': 82, 'y2': 333}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003373.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 232, 'x2': 441, 'y1': 160, 'y2': 329}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003374.jpg', 'width': 480, 'height': 359, 'bboxes': [{'class': 'dog', 'x1': 135, 'x2': 311, 'y1': 118, 'y2': 359}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003430.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 150, 'x2': 355, 'y1': 128, 'y2': 324}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003489.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 280, 'x2': 456, 'y1': 32, 'y2': 257}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003491.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 31, 'x2': 419, 'y1': 15, 'y2': 347}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003508.jpg', 'width': 333, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 269, 'y1': 157, 'y2': 500}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003604.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 62, 'x2': 500, 'y1': 103, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003627.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 135, 'x2': 365, 'y1': 1, 'y2': 333}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003671.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 135, 'x2': 358, 'y1': 6, 'y2': 323}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003681.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 213, 'x2': 284, 'y1': 138, 'y2': 283}, {'class': 'dog', 'x1': 100, 'x2': 206, 'y1': 156, 'y2': 330}, {'class': 'dog', 'x1': 1, 'x2': 121, 'y1': 159, 'y2': 326}, {'class': 'dog', 'x1': 204, 'x2': 368, 'y1': 153, 'y2': 326}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003792.jpg', 'width': 500, 'height': 338, 'bboxes': [{'class': 'dog', 'x1': 16, 'x2': 150, 'y1': 108, 'y2': 260}, {'class': 'dog', 'x1': 178, 'x2': 410, 'y1': 108, 'y2': 288}, {'class': 'dog', 'x1': 351, 'x2': 474, 'y1': 87, 'y2': 267}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003899.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 124, 'x2': 318, 'y1': 117, 'y2': 297}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003921.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 83, 'x2': 398, 'y1': 68, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003935.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 98, 'x2': 347, 'y1': 55, 'y2': 340}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/003969.jpg', 'width': 500, 'height': 334, 'bboxes': [{'class': 'dog', 'x1': 410, 'x2': 440, 'y1': 118, 'y2': 180}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004008.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 133, 'x2': 288, 'y1': 55, 'y2': 310}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004034.jpg', 'width': 375, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 102, 'x2': 375, 'y1': 118, 'y2': 458}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004035.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 231, 'x2': 394, 'y1': 15, 'y2': 297}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004095.jpg', 'width': 500, 'height': 334, 'bboxes': [{'class': 'dog', 'x1': 186, 'x2': 323, 'y1': 97, 'y2': 304}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004137.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 238, 'x2': 486, 'y1': 127, 'y2': 370}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004189.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 171, 'x2': 344, 'y1': 57, 'y2': 280}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004209.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 29, 'x2': 100, 'y1': 97, 'y2': 195}, {'class': 'dog', 'x1': 174, 'x2': 257, 'y1': 117, 'y2': 199}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004232.jpg', 'width': 500, 'height': 263, 'bboxes': [{'class': 'dog', 'x1': 26, 'x2': 101, 'y1': 130, 'y2': 180}, {'class': 'dog', 'x1': 110, 'x2': 159, 'y1': 140, 'y2': 182}, {'class': 'dog', 'x1': 252, 'x2': 320, 'y1': 128, 'y2': 171}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004272.jpg', 'width': 375, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 163, 'x2': 367, 'y1': 146, 'y2': 394}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004318.jpg', 'width': 375, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 365, 'y1': 61, 'y2': 283}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004327.jpg', 'width': 500, 'height': 422, 'bboxes': [{'class': 'dog', 'x1': 12, 'x2': 493, 'y1': 66, 'y2': 415}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004434.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 142, 'x2': 366, 'y1': 193, 'y2': 318}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004548.jpg', 'width': 500, 'height': 367, 'bboxes': [{'class': 'dog', 'x1': 141, 'x2': 414, 'y1': 58, 'y2': 272}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004551.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 493, 'y1': 1, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004565.jpg', 'width': 500, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 3, 'x2': 390, 'y1': 133, 'y2': 497}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004587.jpg', 'width': 333, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 209, 'x2': 322, 'y1': 298, 'y2': 356}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004595.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 159, 'x2': 333, 'y1': 77, 'y2': 358}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004644.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 186, 'y1': 26, 'y2': 260}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004679.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 143, 'x2': 362, 'y1': 79, 'y2': 315}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004737.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 185, 'x2': 500, 'y1': 55, 'y2': 332}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004760.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 210, 'x2': 495, 'y1': 114, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004929.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 215, 'x2': 500, 'y1': 21, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/004992.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 54, 'x2': 480, 'y1': 57, 'y2': 294}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005018.jpg', 'width': 500, 'height': 331, 'bboxes': [{'class': 'dog', 'x1': 191, 'x2': 412, 'y1': 85, 'y2': 308}, {'class': 'dog', 'x1': 23, 'x2': 250, 'y1': 5, 'y2': 198}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005097.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 114, 'x2': 459, 'y1': 36, 'y2': 353}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005121.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 30, 'x2': 205, 'y1': 91, 'y2': 333}, {'class': 'dog', 'x1': 344, 'x2': 500, 'y1': 66, 'y2': 333}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005203.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 118, 'x2': 332, 'y1': 103, 'y2': 351}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005231.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 215, 'x2': 386, 'y1': 109, 'y2': 272}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005303.jpg', 'width': 378, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 72, 'x2': 378, 'y1': 196, 'y2': 470}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005327.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 288, 'x2': 401, 'y1': 210, 'y2': 257}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005408.jpg', 'width': 400, 'height': 300, 'bboxes': [{'class': 'dog', 'x1': 87, 'x2': 335, 'y1': 113, 'y2': 269}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005433.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 139, 'x2': 332, 'y1': 156, 'y2': 280}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005453.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 197, 'y1': 235, 'y2': 330}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005524.jpg', 'width': 334, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 22, 'x2': 334, 'y1': 48, 'y2': 297}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005601.jpg', 'width': 500, 'height': 382, 'bboxes': [{'class': 'dog', 'x1': 2, 'x2': 427, 'y1': 161, 'y2': 382}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005668.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 2, 'x2': 185, 'y1': 131, 'y2': 373}, {'class': 'dog', 'x1': 167, 'x2': 338, 'y1': 117, 'y2': 373}, {'class': 'dog', 'x1': 293, 'x2': 500, 'y1': 112, 'y2': 324}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005752.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 3, 'x2': 433, 'y1': 40, 'y2': 322}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005789.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 237, 'x2': 500, 'y1': 137, 'y2': 347}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005873.jpg', 'width': 333, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 140, 'x2': 245, 'y1': 191, 'y2': 338}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005888.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 277, 'x2': 500, 'y1': 40, 'y2': 373}, {'class': 'dog', 'x1': 13, 'x2': 334, 'y1': 69, 'y2': 375}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005899.jpg', 'width': 500, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 125, 'x2': 379, 'y1': 125, 'y2': 260}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005948.jpg', 'width': 500, 'height': 332, 'bboxes': [{'class': 'dog', 'x1': 166, 'x2': 229, 'y1': 198, 'y2': 230}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005984.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 93, 'x2': 295, 'y1': 200, 'y2': 369}, {'class': 'dog', 'x1': 280, 'x2': 464, 'y1': 215, 'y2': 330}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/005992.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 60, 'x2': 286, 'y1': 147, 'y2': 245}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006065.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 4, 'x2': 500, 'y1': 29, 'y2': 287}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006100.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 141, 'x2': 198, 'y1': 163, 'y2': 200}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006171.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 2, 'x2': 412, 'y1': 166, 'y2': 315}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006174.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 49, 'x2': 388, 'y1': 76, 'y2': 374}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006189.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 112, 'x2': 426, 'y1': 1, 'y2': 330}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006230.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 487, 'y1': 33, 'y2': 356}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006267.jpg', 'width': 500, 'height': 335, 'bboxes': [{'class': 'dog', 'x1': 84, 'x2': 394, 'y1': 156, 'y2': 301}, {'class': 'dog', 'x1': 100, 'x2': 230, 'y1': 72, 'y2': 198}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006366.jpg', 'width': 500, 'height': 384, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 461, 'y1': 31, 'y2': 384}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006434.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 147, 'x2': 500, 'y1': 1, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006474.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 500, 'y1': 28, 'y2': 333}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006556.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 2, 'x2': 500, 'y1': 2, 'y2': 333}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006597.jpg', 'width': 500, 'height': 384, 'bboxes': [{'class': 'dog', 'x1': 19, 'x2': 445, 'y1': 17, 'y2': 305}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006602.jpg', 'width': 356, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 92, 'x2': 205, 'y1': 121, 'y2': 237}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006609.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 214, 'x2': 500, 'y1': 156, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006735.jpg', 'width': 444, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 40, 'x2': 402, 'y1': 24, 'y2': 490}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006805.jpg', 'width': 500, 'height': 401, 'bboxes': [{'class': 'dog', 'x1': 36, 'x2': 447, 'y1': 13, 'y2': 397}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006849.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 51, 'x2': 285, 'y1': 94, 'y2': 195}, {'class': 'dog', 'x1': 236, 'x2': 500, 'y1': 117, 'y2': 274}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006916.jpg', 'width': 500, 'height': 332, 'bboxes': [{'class': 'dog', 'x1': 299, 'x2': 500, 'y1': 73, 'y2': 328}, {'class': 'dog', 'x1': 5, 'x2': 351, 'y1': 87, 'y2': 331}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/006968.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 158, 'x2': 267, 'y1': 215, 'y2': 349}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007011.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 235, 'y1': 46, 'y2': 372}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007089.jpg', 'width': 500, 'height': 356, 'bboxes': [{'class': 'dog', 'x1': 43, 'x2': 468, 'y1': 46, 'y2': 270}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007129.jpg', 'width': 500, 'height': 397, 'bboxes': [{'class': 'dog', 'x1': 82, 'x2': 245, 'y1': 94, 'y2': 397}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007166.jpg', 'width': 333, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 64, 'x2': 287, 'y1': 130, 'y2': 393}, {'class': 'dog', 'x1': 2, 'x2': 333, 'y1': 91, 'y2': 341}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007214.jpg', 'width': 375, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 30, 'x2': 113, 'y1': 351, 'y2': 491}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007322.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 111, 'x2': 360, 'y1': 4, 'y2': 347}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007411.jpg', 'width': 500, 'height': 432, 'bboxes': [{'class': 'dog', 'x1': 59, 'x2': 466, 'y1': 47, 'y2': 413}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007417.jpg', 'width': 500, 'height': 269, 'bboxes': [{'class': 'dog', 'x1': 40, 'x2': 306, 'y1': 67, 'y2': 208}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007474.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 277, 'x2': 433, 'y1': 71, 'y2': 322}, {'class': 'dog', 'x1': 43, 'x2': 327, 'y1': 45, 'y2': 372}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007578.jpg', 'width': 500, 'height': 331, 'bboxes': [{'class': 'dog', 'x1': 187, 'x2': 333, 'y1': 89, 'y2': 306}, {'class': 'dog', 'x1': 350, 'x2': 426, 'y1': 34, 'y2': 113}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007683.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 51, 'x2': 210, 'y1': 248, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007692.jpg', 'width': 500, 'height': 332, 'bboxes': [{'class': 'dog', 'x1': 229, 'x2': 386, 'y1': 102, 'y2': 230}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007749.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 128, 'x2': 500, 'y1': 23, 'y2': 375}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007775.jpg', 'width': 378, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 45, 'x2': 248, 'y1': 89, 'y2': 284}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007791.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 30, 'x2': 440, 'y1': 61, 'y2': 326}, {'class': 'dog', 'x1': 256, 'x2': 470, 'y1': 233, 'y2': 360}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007838.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 230, 'x2': 318, 'y1': 103, 'y2': 264}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007900.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 187, 'x2': 325, 'y1': 116, 'y2': 286}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/007968.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 132, 'x2': 341, 'y1': 1, 'y2': 287}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008005.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 336, 'x2': 426, 'y1': 179, 'y2': 250}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008075.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 298, 'y1': 64, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008079.jpg', 'width': 500, 'height': 335, 'bboxes': [{'class': 'dog', 'x1': 267, 'x2': 356, 'y1': 249, 'y2': 331}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008186.jpg', 'width': 500, 'height': 443, 'bboxes': [{'class': 'dog', 'x1': 2, 'x2': 484, 'y1': 4, 'y2': 442}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008260.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 94, 'x2': 457, 'y1': 71, 'y2': 255}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008272.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 203, 'x2': 366, 'y1': 239, 'y2': 337}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008322.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 100, 'x2': 248, 'y1': 29, 'y2': 294}, {'class': 'dog', 'x1': 235, 'x2': 500, 'y1': 46, 'y2': 310}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008332.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 233, 'x2': 358, 'y1': 202, 'y2': 375}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008346.jpg', 'width': 291, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 28, 'x2': 210, 'y1': 99, 'y2': 487}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008425.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 322, 'x2': 496, 'y1': 258, 'y2': 341}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008467.jpg', 'width': 375, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 68, 'x2': 251, 'y1': 153, 'y2': 433}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008477.jpg', 'width': 500, 'height': 358, 'bboxes': [{'class': 'dog', 'x1': 27, 'x2': 271, 'y1': 118, 'y2': 358}, {'class': 'dog', 'x1': 259, 'x2': 500, 'y1': 63, 'y2': 358}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008530.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 327, 'y1': 32, 'y2': 333}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008558.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 103, 'x2': 295, 'y1': 36, 'y2': 198}, {'class': 'dog', 'x1': 284, 'x2': 475, 'y1': 108, 'y2': 328}, {'class': 'dog', 'x1': 102, 'x2': 284, 'y1': 166, 'y2': 349}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008655.jpg', 'width': 375, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 258, 'x2': 357, 'y1': 328, 'y2': 500}, {'class': 'dog', 'x1': 266, 'x2': 299, 'y1': 205, 'y2': 283}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008738.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 357, 'y1': 137, 'y2': 332}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008819.jpg', 'width': 375, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 131, 'x2': 294, 'y1': 79, 'y2': 450}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008840.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 139, 'x2': 425, 'y1': 2, 'y2': 258}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008841.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 178, 'x2': 249, 'y1': 243, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008900.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 127, 'x2': 199, 'y1': 278, 'y2': 362}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/008980.jpg', 'width': 459, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 53, 'x2': 372, 'y1': 58, 'y2': 428}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009000.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 219, 'x2': 284, 'y1': 168, 'y2': 225}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009042.jpg', 'width': 500, 'height': 334, 'bboxes': [{'class': 'dog', 'x1': 116, 'x2': 406, 'y1': 92, 'y2': 194}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009066.jpg', 'width': 500, 'height': 351, 'bboxes': [{'class': 'dog', 'x1': 98, 'x2': 467, 'y1': 95, 'y2': 351}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009117.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 188, 'x2': 500, 'y1': 27, 'y2': 373}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009185.jpg', 'width': 500, 'height': 411, 'bboxes': [{'class': 'dog', 'x1': 140, 'x2': 433, 'y1': 64, 'y2': 322}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009197.jpg', 'width': 500, 'height': 376, 'bboxes': [{'class': 'dog', 'x1': 104, 'x2': 467, 'y1': 93, 'y2': 357}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009227.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 430, 'x2': 497, 'y1': 133, 'y2': 187}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009327.jpg', 'width': 334, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 67, 'x2': 113, 'y1': 273, 'y2': 334}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009333.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 207, 'x2': 352, 'y1': 202, 'y2': 268}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009418.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 149, 'x2': 264, 'y1': 237, 'y2': 333}, {'class': 'dog', 'x1': 181, 'x2': 251, 'y1': 102, 'y2': 227}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009518.jpg', 'width': 500, 'height': 369, 'bboxes': [{'class': 'dog', 'x1': 125, 'x2': 412, 'y1': 43, 'y2': 356}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009562.jpg', 'width': 357, 'height': 500, 'bboxes': [{'class': 'dog', 'x1': 16, 'x2': 355, 'y1': 38, 'y2': 472}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009587.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 3, 'x2': 500, 'y1': 1, 'y2': 337}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009605.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 89, 'x2': 458, 'y1': 18, 'y2': 369}], 'imageset': 'test'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009668.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 1, 'x2': 321, 'y1': 19, 'y2': 375}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009713.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 114, 'x2': 406, 'y1': 57, 'y2': 344}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009763.jpg', 'width': 500, 'height': 375, 'bboxes': [{'class': 'dog', 'x1': 42, 'x2': 334, 'y1': 32, 'y2': 318}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009797.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 132, 'x2': 238, 'y1': 50, 'y2': 303}, {'class': 'dog', 'x1': 222, 'x2': 396, 'y1': 110, 'y2': 312}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009918.jpg', 'width': 500, 'height': 333, 'bboxes': [{'class': 'dog', 'x1': 118, 'x2': 287, 'y1': 123, 'y2': 333}], 'imageset': 'trainval'}\n",
      "{'filepath': '/home/henri_tomas/projects/voc07/VOCdevkit/VOC2007/JPEGImages/009961.jpg', 'width': 500, 'height': 374, 'bboxes': [{'class': 'dog', 'x1': 69, 'x2': 392, 'y1': 4, 'y2': 345}], 'imageset': 'trainval'}\n"
     ]
    }
   ],
   "source": [
    "#Sanity Test (get_data)\n",
    "\n",
    "from keras_frcnn.simple_parser import get_data\n",
    "\n",
    "all_imgs, classes_count, class_mapping = get_data('/home/henri_tomas/projects/voc07/2007_train.txt')\n",
    "for i in range(len(all_imgs)):\n",
    "    print(all_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
